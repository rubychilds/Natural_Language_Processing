{
 "metadata": {
  "name": "",
  "signature": "sha256:4ec74d67a72c368fcdc1ea675cbb80cdf5e99c6cd928774b48cab0a8845f9a5f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# CH7 Extracting Information from text\n",
      "* How to extract unstructured text into structured data\n",
      "* robust methods for identification of relationships and entities\n",
      "* appropriateness of corpora and how to train and evaluate models\n",
      "## 7.1 Information Extraction\n",
      "* Structured data - regular and predictable organization of entities and relationships\n",
      "* First convert unstructured data of natural lang sentences into a table then can use SQL\n",
      "* Information extraction is the way of getting data from text\n",
      "* Applications like buinsess intelligence, resume harvesting, media anal, sentiment detection, patent search etc\n",
      "## Information Extraction Architecture\n",
      "* Process document by cleaning, segmentation, tokenization\n",
      "* Next use part of speech taggers\n",
      "* Named entity recognition - searches for mentions of interesting entities in each sentence\n",
      "* Relation recognition - search for likely relations between different entities in a text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "def ie_preprocess(document):\n",
      "    sentences = nltk.sent_tokenize(document) # tokenizes sentences\n",
      "    sentences = [nltk.word_toeknize(sentence) for sentence in sentences] # word tokenize\n",
      "    sentences = [nltk.pos_tag(sent) for sent in sentences] # part of speech tagger\n",
      "    return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 7.2 Chunking\n",
      "* Used for entry recognition\n",
      "* Segments and labels miltitoken sequences\n",
      "* High level chunking can inc. multiple words -> chooses subset of tokens\n",
      "## Noun Phrasing Chunking\n",
      "* Search for chunks corresponding to noun phrases\n",
      "* NP-chunks often smaller than noun phrases. Defined as so to not contain NP-chunks\n",
      "* Any prepositional phrases of subordinate clauses that modify a nominal will not be included \n",
      "* Use pos tags to define NP-chunking\n",
      "* Define chunk grammer consisting of rules that indicate how sentences should be chunked"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"the\",'DT'), (\"little\", \"JJ\"),(\"yellow\",\"JJ\"),\n",
      "            (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\",\"DT\"), (\"cat\", \"NN\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = \"NP:{<DT>?<JJ>*<NN>}\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cp = nltk.RegexpParser(grammar)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = cp.parse(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
        "  barked/VBD\n",
        "  at/IN\n",
        "  (NP the/DT cat/NN))\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Tag Patterns\n",
      "* Describe sequences of words\n",
      "* Pos speech tags delimited using angle brackets \\<DT>?\\<JJ\\>*\\<NN\\>\n",
      "* Multiple adjectives <DT>?<JJ.*>*<NN>"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Chunking with Regular Expressions\n",
      "* RegexParser - starts with flat structure, with no tokens are chunked\n",
      "* chunking rules applied in turn, updating structure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = r\"\"\"\n",
      "        NP:{<DT|PP\\$>?<JJ>*<NN>}\n",
      "        {<NNP>+}\n",
      "        \"\"\" \n",
      "    # Matches optional determiner pr possesive pronoun,0+ adjectives, then\n",
      "    # matches 1+ proper noun"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cp = nltk.RegexpParser(grammar)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"Ruby\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), \n",
      "            (\"her\", \"PP$\"), (\"long\",\"JJ\"), (\"golden\",\"JJ\"), (\"hair\", \"NN\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cp.parse(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP Ruby/NNP)\n",
        "  let/VBD\n",
        "  down/RP\n",
        "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nouns = [(\"money\",\"NN\"), (\"market\", \"NN\"), (\"fund\",\"NN\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = \"NP:{<NN><NN>}\" # chunks 2 consecutive nouns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cp = nltk.RegexpParser(grammar)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cp.parse(nouns) # oncce got money market as a chunk we miss fun as a chunk. Should really change grammar to have a +"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S (NP money/NN market/NN) fund/NN)\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exploring Text Corpora"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cp = nltk.RegexpParser('CHUNK: {<V.*><TO><V.*>}')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagged_sents = nltk.corpus.brown.tagged_sents()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for sent in tagged_sents[:10]:\n",
      "    tree = cp.parse(sent)\n",
      "    for subtree in tree.subtrees():\n",
      "        if subtree.node == 'CHUNK':\n",
      "            print subtree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(CHUNK combined/VBN to/TO achieve/VB)\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_chunks(chunk_string, tagged_sents):\n",
      "    try:\n",
      "        cp = nltk.RegexpChunkParser(chunk)\n",
      "        for sent in tagged_sents[:10]:\n",
      "            tree = cp.parse(sent)\n",
      "            for subtree in tree.subtrees():\n",
      "                if subtree.node == 'CHUNK':\n",
      "                    print subtree    \n",
      "    except Exception as e:\n",
      "        print e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Chinking\n",
      "* Exclusions from a chunk -> clink\n",
      "* Process of removing sequence of tokens from a chunk\n",
      "* If spans entire chunk remove whole chunk\n",
      "* If appears mid chunk then remove them"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = r\"\"\"\n",
      "    NP: \n",
      "    {<.*>+} # chunks everything\n",
      "    }<VBD|IN>+{ # chink sequencce of VBD and IN\n",
      "    \"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"the\",\"DT\"), (\"little\",\"JJ\"), (\"yellow\",\"JJ\"),\n",
      "            (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cp = nltk.RegexpParser(grammar)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cp.parse(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
        "  barked/VBD\n",
        "  at/IN\n",
        "  (NP the/DT cat/NN))\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Representing Chunks: Tags versus Trees\n",
      "* Chunking can be represented by trees or tags\n",
      "* IOB tags - common fiel representation. I = inside, B - begin, O- outside\n",
      "* tagged as b is at begging of chunk, then I for inside, and 0 for other\n",
      "* B,I used for chunk types\n",
      "* eg. We PRP B-NP\n",
      "      saw VBD O\n",
      "      the DT B-NP\n",
      "      yellow JJ I-NP\n",
      "      dog NN I-NP"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 7.3 Developing and Evaluation Chunkers\n",
      "## Reading IOB format and the CoNLL- 2000 Chunking Corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wsj = nltk.corpus.treebank.tagged_sents()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wsj[:10] # then we IOB the sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "[('Pierre', 'NNP'),\n",
        " ('Vinken', 'NNP'),\n",
        " (',', ','),\n",
        " ('61', 'CD'),\n",
        " ('years', 'NNS'),\n",
        " ('old', 'JJ'),\n",
        " (',', ','),\n",
        " ('will', 'MD'),\n",
        " ('join', 'VB'),\n",
        " ('the', 'DT')]"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = '''\n",
      "he PRB B-NPM\n",
      "accepted VBD B-VP\n",
      "the DT B-NP\n",
      "position NN I-NP\n",
      "of IN B-VP\n",
      "vice NN B-NP\n",
      "chairman NN I-NMP\n",
      "of IN B-PP\n",
      "Carlyle NNP B-NP\n",
      "Group NNP I-NP\n",
      ", , O\n",
      "a DT B-NP\n",
      "merchant NN I-NP\n",
      "banking NN I-NP\n",
      "concern NN I-NP\n",
      ". . O\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw() # conversion function which builds a tree rep from one to multiline strings"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import conll2000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print conll2000.chunked_sents('train.txt')[99]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (PP Over/IN)\n",
        "  (NP a/DT cup/NN)\n",
        "  (PP of/IN)\n",
        "  (NP coffee/NN)\n",
        "  ,/,\n",
        "  (NP Mr./NNP Stone/NNP)\n",
        "  (VP told/VBD)\n",
        "  (NP his/PRP$ story/NN)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* contains NP chunks, VP chunks such as 'already delivered'\n",
      "* PP chunks such as 'because of'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  Over/IN\n",
        "  (NP a/DT cup/NN)\n",
        "  of/IN\n",
        "  (NP coffee/NN)\n",
        "  ,/,\n",
        "  (NP Mr./NNP Stone/NNP)\n",
        "  told/VBD\n",
        "  (NP his/PRP$ story/NN)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Simple Evaluation and Baselines"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cp = nltk.RegexpParser(\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['NP'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cp.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ChunkParse score:\n",
        "    IOB Accuracy:  43.4%\n",
        "    Precision:      0.0%\n",
        "    Recall:         0.0%\n",
        "    F-Measure:      0.0%\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* more than 1/3 are tagged with O, not in NP chunk\n",
      "* since no tags found, precision, recall etc all zero\n",
      "* use naive regex chunker that looks for letters that are characteristic of noun phrases"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = r'NP:{<[CDJNP].*>+}'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cp = nltk.RegexpParser(grammar)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cp.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ChunkParse score:\n",
        "    IOB Accuracy:  87.7%\n",
        "    Precision:     70.6%\n",
        "    Recall:        67.8%\n",
        "    F-Measure:     69.2%\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* works well, but need data driven approach\n",
      "* use training corpus to find chunk tag that is most likely for pos tag\n",
      "* UnigramChunker - uses unigram tagger to label sentences with chunk tags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class UnigramChunker(nltk.ChunkParserI):\n",
      "    def __init__(self, train_sents): # should be sentences in chunk trees\n",
      "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] \n",
      "                      for sent in train_sents] # conversion to what tagger expects\n",
      "        self.tagger = nltk.UnigramTagger(train_data)\n",
      "        \n",
      "    def parse(self, sentence):\n",
      "        pos_tags = [pos for (word,pos) in sentence]\n",
      "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
      "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
      "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
      "                     in zip(sentence, chunktags)]\n",
      "        return nltk.chunk.conllstr2tree(conlltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_sents = conll2000.chunked_sents('test.txt',chunk_types=['NP'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_sents = conll2000.chunked_sents('train.txt',chunk_types=['NP'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unigram_chunker = UnigramChunker(train_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print unigram_chunker.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'list' object has no attribute 'split'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-108-8165f1ca3833>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0munigram_chunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mC:\\Users\\Ruby\\Anaconda\\lib\\site-packages\\nltk\\chunk\\api.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, gold)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mchunkscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChunkScore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mchunkscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mchunkscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-106-4976b32eb13c>\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     11\u001b[0m         conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n\u001b[0;32m     12\u001b[0m                      in zip(sentence, chunktags)]\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconllstr2tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconlltags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mC:\\Users\\Ruby\\Anaconda\\lib\\site-packages\\nltk\\chunk\\util.pyc\u001b[0m in \u001b[0;36mconllstr2tree\u001b[1;34m(s, chunk_types, top_node)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mlineno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}